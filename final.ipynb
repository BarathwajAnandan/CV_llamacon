{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Model\n",
    "from transformers import AutoProcessor, Llama4ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    output_router_logits=True,         # enable router-softmax output\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "model = Llama4ForConditionalGeneration.from_pretrained(\n",
    "    \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    config=config,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136490/241157874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"router\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Hook Function to Log\n",
    "router_probs = {}\n",
    "def make_hook(layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        # Determine actual sequence length from input\n",
    "        seq_len = input[0].shape[1] if isinstance(input[0], torch.Tensor) else 0\n",
    "\n",
    "        # Handle tensor output\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            out = output[:seq_len] if seq_len > 0 and seq_len < output.shape[0] else output\n",
    "            out = out.detach().cpu()\n",
    "        # Handle objects with 'router_probs' attribute\n",
    "        elif hasattr(output, 'router_probs'):\n",
    "            out = output.router_probs.detach().cpu()\n",
    "        else:\n",
    "            print(f\"[{layer_name}] Unknown output type: {type(output)}\")\n",
    "            out = None\n",
    "\n",
    "        # Store in dictionary if valid\n",
    "        if out is not None:\n",
    "            if layer_name not in router_probs:\n",
    "                router_probs[layer_name] = []\n",
    "            router_probs[layer_name].append(out)\n",
    "\n",
    "    return hook_fn\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if \"router\" in name.lower():\n",
    "        module.register_forward_hook(make_hook(name))\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136490/1683666718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Time the processing part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstart_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m tokenized = processor.apply_chat_template(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "#ONE TIME INFERENCE\n",
    "import time\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"翻译成英文：你好，我叫巴拉特\" },\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Start timing\n",
    "start_total = time.time()\n",
    "\n",
    "# Time the processing part\n",
    "start_process = time.time()\n",
    "tokenized = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    ")\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "process_time = time.time() - start_process\n",
    "num_input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "print(f\"Number of input tokens: {num_input_tokens}\")\n",
    "# Time the generation part\n",
    "start_generate = time.time()\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "generate_time = time.time() - start_generate\n",
    "\n",
    "# Time the decoding part\n",
    "start_decode = time.time()\n",
    "generated_tokens = outputs.sequences[0, inputs.input_ids.shape[1]:]\n",
    "decoded_text = processor.decode(generated_tokens, skip_special_tokens=True)\n",
    "decode_time = time.time() - start_decode\n",
    "\n",
    "# Calculate total time\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of output tokens: {len(generated_tokens)}\")\n",
    "print(f\"Processing time: {process_time:.4f} seconds\")\n",
    "print(f\"Generation time: {generate_time:.4f} seconds\")\n",
    "print(f\"Decoding time: {decode_time:.4f} seconds\")\n",
    "print(f\"Total time: {total_time:.4f} seconds\")\n",
    "print(\"\\nGenerated text:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
